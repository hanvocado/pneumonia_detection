{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOjTujJkecnhwqtVph+PS6T",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hanvocado/pneumonia_detection/blob/linh/src/model/cnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p4ySRn_ewJkk"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import time\n",
        "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# C·∫§U H√åNH & LOAD D·ªÆ LI·ªÜU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "#print(f\"ƒêang ch·∫°y tr√™n thi·∫øt b·ªã: {device}\")\n",
        "\n",
        "base_dir = '/content/pneumonia_detection/data_processed_new'\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "LEARNING_RATE = 0.001\n",
        "NUM_EPOCHS = 12\n",
        "\n",
        "data_transforms = transforms.Compose([\n",
        "    transforms.Grayscale(num_output_channels=1),\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "\n",
        "try:\n",
        "    train_dataset = datasets.ImageFolder(os.path.join(base_dir, 'train'), transform=data_transforms)\n",
        "    val_dataset   = datasets.ImageFolder(os.path.join(base_dir, 'val'), transform=data_transforms)\n",
        "    test_dataset  = datasets.ImageFolder(os.path.join(base_dir, 'test'), transform=data_transforms)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "    val_loader   = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "    test_loader  = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "    print(f\"Class mapping: {train_dataset.class_to_idx}\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"L·ªñI: Kh√¥ng t√¨m th·∫•y folder t·∫°i {base_dir}\")\n",
        "\n",
        "# X√ÇY D·ª∞NG MODEL CNN\n",
        "\n",
        "class BasicCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(BasicCNN, self).__init__()\n",
        "        # Block 1\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        # Block 2\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(64)\n",
        "        # Block 3\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "        self.bn3 = nn.BatchNorm2d(128)\n",
        "        # Block 4\n",
        "        self.conv4 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
        "        self.bn4 = nn.BatchNorm2d(256)\n",
        "\n",
        "        self.flatten_size = 256 * 14 * 14\n",
        "\n",
        "        self.fc1 = nn.Linear(self.flatten_size, 512)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "        self.fc2 = nn.Linear(512, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(torch.relu(self.bn1(self.conv1(x))))\n",
        "        x = self.pool(torch.relu(self.bn2(self.conv2(x))))\n",
        "        x = self.pool(torch.relu(self.bn3(self.conv3(x))))\n",
        "        x = self.pool(torch.relu(self.bn4(self.conv4(x))))\n",
        "        x = x.view(-1, self.flatten_size)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "model = BasicCNN().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "# V√íNG L·∫∂P HU·∫§N LUY·ªÜN\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "val_accuracies = []\n",
        "\n",
        "print(\"\\nB·∫ÆT ƒê·∫¶U HU·∫§N LUY·ªÜN MODEL\")\n",
        "start_time = time.time()\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    # --- TRAINING ---\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    avg_train_loss = running_loss / len(train_loader)\n",
        "    train_losses.append(avg_train_loss)\n",
        "\n",
        "    # VALIDATION\n",
        "    model.eval()\n",
        "    running_val_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in val_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "\n",
        "            loss = criterion(outputs, labels)\n",
        "            running_val_loss += loss.item()\n",
        "\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    avg_val_loss = running_val_loss / len(val_loader)\n",
        "    val_losses.append(avg_val_loss)\n",
        "\n",
        "    val_acc = 100 * correct / total\n",
        "    val_accuracies.append(val_acc)\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{NUM_EPOCHS}] | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f} | Val Acc: {val_acc:.2f}%\")\n",
        "\n",
        "total_time = (time.time() - start_time) / 60\n",
        "print(f\"\\n ƒê√£ hu·∫•n luy·ªán xong trong {total_time:.2f} ph√∫t!\")\n",
        "\n",
        "# 4. ƒê√ÅNH GI√Å & V·∫º BI·ªÇU ƒê·ªí\n",
        "def evaluate_and_visualize(model, loader, device, train_losses, val_losses, val_accuracies):\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    all_probs = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            probs = F.softmax(outputs, dim=1)\n",
        "\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_probs.extend(probs[:, 1].cpu().numpy())\n",
        "\n",
        "    # T√≠nh to√°n ch·ªâ s·ªë\n",
        "    cm = confusion_matrix(all_labels, all_preds)\n",
        "    tn, fp, fn, tp = cm.ravel()\n",
        "    report = classification_report(all_labels, all_preds, target_names=['NORMAL', 'PNEUMONIA'])\n",
        "    fpr, tpr, thresholds = roc_curve(all_labels, all_probs)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "\n",
        "    # BI·ªÇU ƒê·ªí 1: LEARNING CURVE\n",
        "    epochs_range = range(1, len(train_losses) + 1)\n",
        "\n",
        "    fig, ax1 = plt.subplots(figsize=(12, 6))\n",
        "\n",
        "    color = 'tab:red'\n",
        "    ax1.set_xlabel('Epoch')\n",
        "    ax1.set_ylabel('Loss Value', color=color)\n",
        "\n",
        "    ax1.plot(epochs_range, train_losses, color=color, marker='o', label='Train Loss')\n",
        "    ax1.plot(epochs_range, val_losses, color='tab:orange', marker='x', linestyle='--', label='Val Loss')\n",
        "\n",
        "    ax1.tick_params(axis='y', labelcolor=color)\n",
        "    ax1.grid(True, linestyle='--', alpha=0.6)\n",
        "    ax1.set_xticks(epochs_range)\n",
        "    ax1.legend(loc='upper left')\n",
        "\n",
        "    ax2 = ax1.twinx()\n",
        "    color = 'tab:green'\n",
        "    ax2.set_ylabel('Accuracy (%)', color=color)\n",
        "    ax2.plot(epochs_range, val_accuracies, color=color, marker='s', label='Val Accuracy')\n",
        "    ax2.tick_params(axis='y', labelcolor=color)\n",
        "    ax2.legend(loc='upper right')\n",
        "\n",
        "    plt.title('Learning Curve: Train/Val Loss & Validation Accuracy')\n",
        "    plt.show()\n",
        "\n",
        "    # BI·ªÇU ƒê·ªí 2: CONFUSION MATRIX\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,\n",
        "                xticklabels=['NORMAL', 'PNEUMONIA'],\n",
        "                yticklabels=['NORMAL', 'PNEUMONIA'], annot_kws={\"size\": 16})\n",
        "    plt.xlabel('D·ª± ƒëo√°n (Predicted)', fontsize=12)\n",
        "    plt.ylabel('Th·ª±c t·∫ø (Ground Truth)', fontsize=12)\n",
        "    plt.title(f'Confusion Matrix (Test Accuracy: {(tp+tn)/(tp+tn+fp+fn):.2%})', fontsize=14)\n",
        "    plt.show()\n",
        "\n",
        "    # BI·ªÇU ƒê·ªí 3: ROC CURVE\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.4f})')\n",
        "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('False Positive Rate', fontsize=12)\n",
        "    plt.ylabel('True Positive Rate', fontsize=12)\n",
        "    plt.title('ROC Curve Analysis', fontsize=14)\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "    print(\"\\n\" + \"=\"*40)\n",
        "    print(\"üìä B√ÅO C√ÅO HI·ªÜU SU·∫§T MODEL\")\n",
        "    print(\"=\"*40)\n",
        "    print(report)\n",
        "    print(\"-\" * 40)\n",
        "    print(f\"‚úÖ AUC Score: {roc_auc:.4f}\")\n",
        "    print(f\"‚ùå False Negatives (S√≥t b·ªánh): {fn} ca\")\n",
        "    print(f\"‚ö†Ô∏è False Positives (B√°o nh·∫ßm): {fp} ca\")\n",
        "    print(\"=\"*40)\n",
        "\n",
        "evaluate_and_visualize(model, test_loader, device, train_losses, val_losses, val_accuracies)\n",
        "\n",
        "# L∆ØU MODEL\n",
        "torch.save(model.state_dict(), 'pneumonia_cnn_12epochs_lan3.pth')"
      ]
    }
  ]
}